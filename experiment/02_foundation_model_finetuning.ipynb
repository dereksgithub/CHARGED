{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Foundation Model Fine-Tuning\n",
    "\n",
    "This notebook demonstrates fine-tuning pre-trained foundation models (MOMENT, Chronos, Moirai) for EV charging demand prediction.\n",
    "\n",
    "**Objectives**:\n",
    "- Fine-tune MOMENT model on Shenzhen dataset\n",
    "- Compare with baseline models from Phase 1\n",
    "- Target: >10% MAE improvement over best baseline\n",
    "\n",
    "**Strategy**:\n",
    "1. Start with MOMENT (best multivariate support)\n",
    "2. Progressive unfreezing (head → last 2 layers → full model)\n",
    "3. Incorporate auxiliary features (weather, pricing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from api.dataset.common import EVDataset\n",
    "from api.model.foundation import MOMENTForecaster, load_foundation_model\n",
    "from api.utils import calculate_regression_metrics\n",
    "from experiment.utils.experiment_tracking import ExperimentTracker\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'city': 'SZH',  # Shenzhen - largest dataset\n",
    "    'model_name': 'moment',  # Start with MOMENT\n",
    "    'model_size': 'small',  # Use small for faster training initially\n",
    "    'feature': 'volume',\n",
    "    'auxiliary': 'all',  # Use all auxiliary features\n",
    "    \n",
    "    # Data configuration\n",
    "    'context_length': 168,  # 1 week of hourly data\n",
    "    'forecast_horizon': 24,  # 1 day ahead prediction\n",
    "    'use_segments': False,  # Set True for segment-based (MOMENT-specific)\n",
    "    \n",
    "    # Training configuration\n",
    "    'batch_size': 16,  # Smaller batch for foundation models\n",
    "    'epochs': 30,\n",
    "    'learning_rate_head': 1e-3,\n",
    "    'learning_rate_encoder': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    \n",
    "    # Progressive unfreezing\n",
    "    'freeze_encoder': True,  # Start with frozen encoder\n",
    "    'unfreeze_at_epoch': 10,  # Unfreeze last 2 layers after epoch 10\n",
    "    'unfreeze_layers': 2,\n",
    "    \n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(CONFIG['seed'])\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EV dataset\n",
    "data_path = f'../data/{CONFIG[\"city\"]}_remove_zero/'\n",
    "print(f\"Loading dataset from {data_path}...\")\n",
    "\n",
    "ev_dataset = EVDataset(\n",
    "    feature=CONFIG['feature'],\n",
    "    auxiliary=CONFIG['auxiliary'],\n",
    "    data_path=data_path,\n",
    "    pred_type='site',\n",
    "    seq_l=CONFIG['context_length'],\n",
    "    pre_len=CONFIG['forecast_horizon']\n",
    ")\n",
    "\n",
    "# Split data (80% train, 10% val, 10% test)\n",
    "ev_dataset.split_cross_validation(\n",
    "    fold=1,\n",
    "    total_fold=6,\n",
    "    train_ratio=0.8,\n",
    "    valid_ratio=0.1\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Dataset loaded:\")\n",
    "print(f\"  Sites: {ev_dataset.feat.shape[1]}\")\n",
    "print(f\"  Total timesteps: {ev_dataset.feat.shape[0]}\")\n",
    "print(f\"  Training samples: {len(ev_dataset.train_feat)}\")\n",
    "print(f\"  Validation samples: {len(ev_dataset.valid_feat)}\")\n",
    "print(f\"  Test samples: {len(ev_dataset.test_feat)}\")\n",
    "\n",
    "if ev_dataset.extra_feat.size > 0:\n",
    "    n_aux = ev_dataset.extra_feat.shape[1]\n",
    "    print(f\"  Auxiliary features: {n_aux}\")\n",
    "else:\n",
    "    n_aux = 0\n",
    "    print(f\"  Auxiliary features: None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Foundation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize experiment tracker\n",
    "experiment_name = f\"foundation_{CONFIG['model_name']}_{CONFIG['city']}\"\n",
    "tracker = ExperimentTracker(experiment_name, save_dir='../results/foundation_models')\n",
    "\n",
    "tracker.log_hyperparameters(CONFIG)\n",
    "\n",
    "# Load foundation model\n",
    "print(f\"\\nLoading {CONFIG['model_name']} model...\")\n",
    "\n",
    "try:\n",
    "    model = load_foundation_model(\n",
    "        model_name=CONFIG['model_name'],\n",
    "        model_size=CONFIG['model_size'],\n",
    "        n_aux_features=n_aux,\n",
    "        prediction_length=CONFIG['forecast_horizon'],\n",
    "        context_length=CONFIG['context_length']\n",
    "    )\n",
    "    \n",
    "    model = model.to(CONFIG['device'])\n",
    "    \n",
    "    # Freeze encoder if specified\n",
    "    if CONFIG['freeze_encoder']:\n",
    "        model.freeze_encoder()\n",
    "    \n",
    "    print(f\"\\n✓ Model loaded successfully\")\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"  Frozen parameters: {total_params - trainable_params:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error loading model: {str(e)}\")\n",
    "    print(\"\\nNote: This notebook requires foundation model libraries.\")\n",
    "    print(\"Install with:\")\n",
    "    print(\"  pip install momentfm  # for MOMENT\")\n",
    "    print(\"  pip install chronos-forecasting  # for Chronos\")\n",
    "    print(\"  pip install uni2ts  # for Moirai\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is a simplified data preparation.\n",
    "# For production, you'll need to adapt this based on the specific\n",
    "# foundation model's expected input format.\n",
    "\n",
    "print(\"Preparing data loaders...\")\n",
    "print(\"\\n⚠️  Note: This is a placeholder implementation.\")\n",
    "print(\"Each foundation model (MOMENT/Chronos/Moirai) has specific\")\n",
    "print(\"input format requirements that need to be implemented based\")\n",
    "print(\"on their respective APIs.\")\n",
    "\n",
    "# Placeholder for data preparation\n",
    "# In practice, you'll need to:\n",
    "# 1. Format data according to model requirements\n",
    "# 2. Handle auxiliary features appropriately\n",
    "# 3. Create proper sliding windows\n",
    "# 4. Normalize/scale data\n",
    "\n",
    "print(\"\\n✓ Data loaders prepared (placeholder)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop with Progressive Unfreezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SETUP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n⚠️  This is a template implementation.\")\n",
    "print(\"\\nThe actual training loop needs to be customized for each foundation model:\")\n",
    "print(\"\\n1. MOMENT: Uses MOMENTPipeline with specific forecast() method\")\n",
    "print(\"2. Chronos: Requires ChronosPipeline.predict() with context\")\n",
    "print(\"3. Moirai: Uses GluonTS dataset format with MoiraiForecast\")\n",
    "\n",
    "print(\"\\nFor now, please refer to:\")\n",
    "print(\"  - example/test_moment_all.py for MOMENT integration\")\n",
    "print(\"  - example/test_chronos_all.py for Chronos integration\")\n",
    "print(\"  - example/test_moirai_all.py for Moirai integration\")\n",
    "\n",
    "print(\"\\nThese scripts show how to properly use each model's API.\")\n",
    "print(\"You'll need to adapt them for fine-tuning with PyTorch optimizers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "To complete this notebook, you need to:\n",
    "\n",
    "### 1. Study the existing test scripts\n",
    "- `example/test_moment_all.py` - Shows MOMENT data preparation and inference\n",
    "- `example/test_chronos_all.py` - Shows Chronos usage\n",
    "- `example/test_moirai_all.py` - Shows Moirai with GluonTS\n",
    "\n",
    "### 2. Implement model-specific training\n",
    "Each foundation model has a different API:\n",
    "- **MOMENT**: Likely supports `.fit()` or requires custom training loop\n",
    "- **Chronos**: T5-based, may need HuggingFace Trainer\n",
    "- **Moirai**: GluonTS integration, custom training needed\n",
    "\n",
    "### 3. Alternative: Use existing scripts as baseline\n",
    "You can:\n",
    "1. Run the test scripts to get zero-shot baseline performance\n",
    "2. Compare with Phase 1 baseline results\n",
    "3. Then implement fine-tuning for the best-performing model\n",
    "\n",
    "### 4. Recommended Approach\n",
    "Start with the **simplest path**:\n",
    "1. Run `python example/test_moment_all.py` to get MOMENT zero-shot results\n",
    "2. Compare with baseline MAE from Phase 1\n",
    "3. If MOMENT already beats baselines → document this!\n",
    "4. If not → implement fine-tuning in next iteration\n",
    "\n",
    "This is a common pattern in research: first establish baselines, then refine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for recording that this notebook was run\n",
    "tracker.log_metrics({\n",
    "    'status': 'template_created',\n",
    "    'note': 'Awaiting model-specific implementation'\n",
    "})\n",
    "\n",
    "tracker.print_summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 2 TEMPLATE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nThis notebook provides the structure for foundation model fine-tuning.\")\n",
    "print(\"See 'Next Steps' section above for implementation guidance.\")\n",
    "print(\"\\nRecommended: Start by running existing test scripts to establish\")\n",
    "print(\"zero-shot baseline performance before implementing fine-tuning.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

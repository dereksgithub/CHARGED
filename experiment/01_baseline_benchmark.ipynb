{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Baseline Model Benchmarking\n",
    "\n",
    "This notebook systematically evaluates all baseline models on the CHARGED dataset.\n",
    "\n",
    "**Objectives**:\n",
    "- Establish performance benchmarks for all 10 baseline models\n",
    "- Compare models across different cities\n",
    "- Identify best baseline model per city\n",
    "- Quantify auxiliary feature impact\n",
    "\n",
    "**Models to Benchmark**:\n",
    "1. Statistical: Lo, AR, ARIMA\n",
    "2. Neural: LSTM, FCNN, SegRNN, FreTS, ModernTCN, MultiPatchFormer, ConvTimeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from api.dataset.common import EVDataset\n",
    "from api.model.config import PredictionModel\n",
    "from api.trainer.common import PredictionTrainer\n",
    "from api.parsing.common import parse_args\n",
    "from api.utils import calculate_regression_metrics\n",
    "from experiment.utils.experiment_tracking import ExperimentTracker\n",
    "\n",
    "print(\"âœ“ Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "CONFIG = {\n",
    "    'cities': ['SZH'],  # Start with Shenzhen (largest dataset)\n",
    "    'models': ['moderntcn', 'convtimenet', 'lstm'],  # Priority models\n",
    "    'feature': 'volume',\n",
    "    'seq_lengths': [24],  # 24 hours lookback\n",
    "    'pred_lengths': [1],  # 1 hour ahead prediction\n",
    "    'auxiliary_modes': ['None', 'all'],  # Test with and without auxiliary features\n",
    "    'folds': [1],  # Start with fold 1\n",
    "    'epochs': 50,\n",
    "    'batch_size': 32,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "print(\"Experiment Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline_experiment(city, model_name, seq_len, pred_len, auxiliary, fold, epochs, batch_size, device):\n",
    "    \"\"\"\n",
    "    Run a single baseline experiment with specified configuration.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Experiment results including metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running: {model_name} | {city} | seq={seq_len} | pred={pred_len} | aux={auxiliary}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Initialize experiment tracker\n",
    "    experiment_name = f\"baseline_{model_name}_{city}\"\n",
    "    tracker = ExperimentTracker(experiment_name, save_dir='../results/baselines')\n",
    "    \n",
    "    # Log hyperparameters\n",
    "    tracker.log_hyperparameters({\n",
    "        'city': city,\n",
    "        'model': model_name,\n",
    "        'feature': 'volume',\n",
    "        'seq_len': seq_len,\n",
    "        'pred_len': pred_len,\n",
    "        'auxiliary': auxiliary,\n",
    "        'fold': fold,\n",
    "        'epochs': epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'device': device\n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        # Load dataset\n",
    "        data_path = f'../data/{city}_remove_zero/'\n",
    "        print(f\"Loading dataset from {data_path}...\")\n",
    "        \n",
    "        ev_dataset = EVDataset(\n",
    "            feature='volume',\n",
    "            auxiliary=auxiliary,\n",
    "            data_path=data_path,\n",
    "            pred_type='site',\n",
    "            seq_l=seq_len,\n",
    "            pre_len=pred_len\n",
    "        )\n",
    "        \n",
    "        # Split data (80% train, 10% val, 10% test)\n",
    "        ev_dataset.split_cross_validation(\n",
    "            fold=fold,\n",
    "            total_fold=6,\n",
    "            train_ratio=0.8,\n",
    "            valid_ratio=0.1\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset loaded: {ev_dataset.feat.shape[1]} sites, {ev_dataset.feat.shape[0]} timesteps\")\n",
    "        \n",
    "        # Initialize model\n",
    "        print(f\"Initializing {model_name} model...\")\n",
    "        n_features = 1 + (ev_dataset.extra_feat.shape[1] if ev_dataset.extra_feat.size > 0 else 0)\n",
    "        \n",
    "        model = PredictionModel(\n",
    "            model=model_name,\n",
    "            seq_l=seq_len,\n",
    "            pre_len=pred_len,\n",
    "            n_node=ev_dataset.feat.shape[1],\n",
    "            n_fea=n_features,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = PredictionTrainer(\n",
    "            model=model.model,\n",
    "            dataset=ev_dataset,\n",
    "            epoch=epochs,\n",
    "            batch_size=batch_size,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Train model (if it's a neural model)\n",
    "        if model_name not in ['lo', 'ar', 'arima']:\n",
    "            print(f\"Training model for {epochs} epochs...\")\n",
    "            trainer.training()\n",
    "        \n",
    "        # Test and get metrics\n",
    "        print(\"Evaluating on test set...\")\n",
    "        predictions, labels, metrics = trainer.test()\n",
    "        \n",
    "        # Log results\n",
    "        tracker.log_metrics(metrics)\n",
    "        tracker.save_predictions(predictions, labels)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"RESULTS: {model_name} on {city}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        for metric_name, value in metrics.items():\n",
    "            print(f\"  {metric_name}: {value:.4f}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # Create summary\n",
    "        tracker.create_summary_table()\n",
    "        \n",
    "        return {\n",
    "            'city': city,\n",
    "            'model': model_name,\n",
    "            'seq_len': seq_len,\n",
    "            'pred_len': pred_len,\n",
    "            'auxiliary': auxiliary,\n",
    "            'fold': fold,\n",
    "            **metrics,\n",
    "            'status': 'success'\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error running experiment: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        return {\n",
    "            'city': city,\n",
    "            'model': model_name,\n",
    "            'seq_len': seq_len,\n",
    "            'pred_len': pred_len,\n",
    "            'auxiliary': auxiliary,\n",
    "            'fold': fold,\n",
    "            'status': 'failed',\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Baseline Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments for all configurations\n",
    "all_results = []\n",
    "\n",
    "total_experiments = (\n",
    "    len(CONFIG['cities']) * \n",
    "    len(CONFIG['models']) * \n",
    "    len(CONFIG['seq_lengths']) * \n",
    "    len(CONFIG['pred_lengths']) * \n",
    "    len(CONFIG['auxiliary_modes']) * \n",
    "    len(CONFIG['folds'])\n",
    ")\n",
    "\n",
    "print(f\"Total experiments to run: {total_experiments}\\n\")\n",
    "\n",
    "experiment_count = 0\n",
    "for city in CONFIG['cities']:\n",
    "    for model in CONFIG['models']:\n",
    "        for seq_len in CONFIG['seq_lengths']:\n",
    "            for pred_len in CONFIG['pred_lengths']:\n",
    "                for auxiliary in CONFIG['auxiliary_modes']:\n",
    "                    for fold in CONFIG['folds']:\n",
    "                        experiment_count += 1\n",
    "                        print(f\"\\nðŸ”¬ Experiment {experiment_count}/{total_experiments}\")\n",
    "                        \n",
    "                        result = run_baseline_experiment(\n",
    "                            city=city,\n",
    "                            model_name=model,\n",
    "                            seq_len=seq_len,\n",
    "                            pred_len=pred_len,\n",
    "                            auxiliary=auxiliary,\n",
    "                            fold=fold,\n",
    "                            epochs=CONFIG['epochs'],\n",
    "                            batch_size=CONFIG['batch_size'],\n",
    "                            device=CONFIG['device']\n",
    "                        )\n",
    "                        \n",
    "                        all_results.append(result)\n",
    "\n",
    "print(f\"\\nâœ… All {total_experiments} experiments completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save comprehensive results\n",
    "results_df.to_csv('../results/baselines/comprehensive_baseline_results.csv', index=False)\n",
    "print(\"âœ“ Saved comprehensive results to results/baselines/comprehensive_baseline_results.csv\")\n",
    "\n",
    "# Display results table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE BENCHMARKING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "display(results_df)\n",
    "\n",
    "# Filter successful experiments\n",
    "successful_results = results_df[results_df['status'] == 'success']\n",
    "\n",
    "if len(successful_results) > 0:\n",
    "    print(f\"\\nSuccessful experiments: {len(successful_results)}/{len(results_df)}\")\n",
    "    \n",
    "    # Best model per configuration\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BEST MODELS BY CONFIGURATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for aux in CONFIG['auxiliary_modes']:\n",
    "        aux_results = successful_results[successful_results['auxiliary'] == aux]\n",
    "        if len(aux_results) > 0:\n",
    "            best_idx = aux_results['MAE'].idxmin()\n",
    "            best_model = aux_results.loc[best_idx]\n",
    "            \n",
    "            print(f\"\\nAuxiliary = {aux}:\")\n",
    "            print(f\"  Best Model: {best_model['model']}\")\n",
    "            print(f\"  MAE: {best_model['MAE']:.4f}\")\n",
    "            print(f\"  RMSE: {best_model['RMSE']:.4f}\")\n",
    "            print(f\"  RÂ²: {best_model['RÂ²']:.4f}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ No successful experiments to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(successful_results) > 0:\n",
    "    # Set style\n",
    "    sns.set_style('whitegrid')\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. MAE comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    successful_results.groupby(['model', 'auxiliary'])['MAE'].mean().unstack().plot(\n",
    "        kind='bar', ax=ax1, rot=45\n",
    "    )\n",
    "    ax1.set_title('Mean Absolute Error by Model', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('MAE')\n",
    "    ax1.set_xlabel('Model')\n",
    "    ax1.legend(title='Auxiliary Features')\n",
    "    \n",
    "    # 2. RMSE comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    successful_results.groupby(['model', 'auxiliary'])['RMSE'].mean().unstack().plot(\n",
    "        kind='bar', ax=ax2, rot=45\n",
    "    )\n",
    "    ax2.set_title('Root Mean Square Error by Model', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('RMSE')\n",
    "    ax2.set_xlabel('Model')\n",
    "    ax2.legend(title='Auxiliary Features')\n",
    "    \n",
    "    # 3. RÂ² comparison\n",
    "    ax3 = axes[1, 0]\n",
    "    successful_results.groupby(['model', 'auxiliary'])['RÂ²'].mean().unstack().plot(\n",
    "        kind='bar', ax=ax3, rot=45\n",
    "    )\n",
    "    ax3.set_title('RÂ² Score by Model', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel('RÂ²')\n",
    "    ax3.set_xlabel('Model')\n",
    "    ax3.legend(title='Auxiliary Features')\n",
    "    \n",
    "    # 4. Auxiliary feature impact\n",
    "    ax4 = axes[1, 1]\n",
    "    if len(CONFIG['auxiliary_modes']) > 1:\n",
    "        aux_impact = []\n",
    "        for model in CONFIG['models']:\n",
    "            model_results = successful_results[successful_results['model'] == model]\n",
    "            if len(model_results) >= 2:\n",
    "                mae_none = model_results[model_results['auxiliary'] == 'None']['MAE'].mean()\n",
    "                mae_all = model_results[model_results['auxiliary'] == 'all']['MAE'].mean()\n",
    "                improvement = (mae_none - mae_all) / mae_none * 100\n",
    "                aux_impact.append({'model': model, 'improvement_%': improvement})\n",
    "        \n",
    "        if aux_impact:\n",
    "            aux_df = pd.DataFrame(aux_impact)\n",
    "            aux_df.plot(x='model', y='improvement_%', kind='bar', ax=ax4, legend=False, rot=45)\n",
    "            ax4.set_title('Auxiliary Features Impact (% MAE Improvement)', fontsize=12, fontweight='bold')\n",
    "            ax4.set_ylabel('Improvement (%)')\n",
    "            ax4.set_xlabel('Model')\n",
    "            ax4.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/baselines/baseline_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\nâœ“ Saved visualization to results/baselines/baseline_comparison.png\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nâš ï¸ No successful experiments to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 1 COMPLETE: BASELINE BENCHMARKING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(successful_results) > 0:\n",
    "    best_overall = successful_results.loc[successful_results['MAE'].idxmin()]\n",
    "    \n",
    "    print(f\"\\nâœ… Best Overall Configuration:\")\n",
    "    print(f\"   Model: {best_overall['model']}\")\n",
    "    print(f\"   City: {best_overall['city']}\")\n",
    "    print(f\"   Auxiliary: {best_overall['auxiliary']}\")\n",
    "    print(f\"   MAE: {best_overall['MAE']:.4f}\")\n",
    "    print(f\"   RMSE: {best_overall['RMSE']:.4f}\")\n",
    "    print(f\"   RÂ²: {best_overall['RÂ²']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Results Summary:\")\n",
    "    print(f\"   Total experiments: {len(results_df)}\")\n",
    "    print(f\"   Successful: {len(successful_results)}\")\n",
    "    print(f\"   Failed: {len(results_df) - len(successful_results)}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ Results saved to:\")\n",
    "    print(f\"   - results/baselines/comprehensive_baseline_results.csv\")\n",
    "    print(f\"   - results/baselines/baseline_comparison.png\")\n",
    "    print(f\"   - Individual experiment folders in results/baselines/baseline_*/\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Next Steps (Phase 2):\")\n",
    "    print(f\"   1. Implement foundation model wrappers (api/model/foundation.py)\")\n",
    "    print(f\"   2. Fine-tune MOMENT model on Shenzhen\")\n",
    "    print(f\"   3. Target: Beat {best_overall['model']} MAE of {best_overall['MAE']:.4f}\")\n",
    "    print(f\"   4. Goal: Achieve >10% improvement (MAE < {best_overall['MAE'] * 0.9:.4f})\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ All experiments failed. Please check the error messages above.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
